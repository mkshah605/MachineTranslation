{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bugs to fix:\n",
    "\n",
    "\n",
    "# cpu to gpu\n",
    "# why is loss rising?\n",
    "# Attention layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as t\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from itertools import repeat\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "device = t.device(\"cpu\")\n",
    "# device = t.device(\"mps\")\n",
    "# device = t.device(\"mps\" if t.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Training Files\n",
    "\n",
    "directory = \"corpus_files\"\n",
    "\n",
    "en_corpus_file = \"train_en.txt\"\n",
    "en_file = open(directory + \"/\" + en_corpus_file, \"r\", encoding=\"utf-8\")\n",
    "en_sentences = en_file.read()\n",
    "en_file.close()\n",
    "en_sentences = en_sentences.split('\\n')\n",
    "# print(en_sentences[0])\n",
    "\n",
    "gu_corpus_file = \"train_gu.txt\"\n",
    "gu_file = open(directory + \"/\" + gu_corpus_file, \"r\", encoding=\"utf-8\")\n",
    "gu_sentences = gu_file.read()\n",
    "gu_file.close()\n",
    "gu_sentences = gu_sentences.split('\\n')\n",
    "# print(gu_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65001\n",
      "65001\n"
     ]
    }
   ],
   "source": [
    "# How many English and Gujarati sentences? Confirm they're the same amount\n",
    "print(len(en_sentences))\n",
    "print(len(gu_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zucchini.\n",
      "હ્યુસ્ટનમાં\n"
     ]
    }
   ],
   "source": [
    "# Load in Vocab Files\n",
    "\n",
    "en_tokens_file = \"vocab_en.txt\"\n",
    "en_file = open(directory + \"/\" + en_tokens_file, \"r\")\n",
    "en_words = en_file.read()\n",
    "en_file.close()\n",
    "en_words =  en_words.split('\\n')\n",
    "\n",
    "# strip leading spaces from the word strings\n",
    "# add the pad token\n",
    "en_words.append(\"</pad>\")\n",
    "print(max(en_words))\n",
    "\n",
    "gu_tokens_file = \"vocab_gu.txt\"\n",
    "gu_file = open(directory + \"/\" + gu_tokens_file, \"r\")\n",
    "gu_words = gu_file.read()\n",
    "gu_file.close()\n",
    "gu_words =  gu_words.split('\\n')\n",
    "\n",
    "# get rid of the zero-byte characters\n",
    "gu_words = [word.replace(\"\\u200b\",\"\") for word in gu_words]\n",
    "# strip leading spaces from the word strings\n",
    "gu_words.append(\"</પેડ>\")\n",
    "print(max(gu_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n",
    "# build a dictionary\n",
    "# word to index for both langs\n",
    "\n",
    "eng_word2index = {}\n",
    "eng_index2word = {}\n",
    "for i, word in enumerate(en_words):\n",
    "    eng_word2index[word] = i\n",
    "    eng_index2word[i] = word\n",
    "\n",
    "guj_word2index = {}\n",
    "guj_index2word = {}\n",
    "for i, word in enumerate(gu_words):\n",
    "    guj_word2index[word] = i\n",
    "    guj_index2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get collection of tokenized indexes for each string\n",
    "\n",
    "# english [[1 sentence words tokenized], []]\n",
    "# guj [[1 sentence words tokenized], []]\n",
    "\n",
    "# english word to index\n",
    "max_eng_sent_len = 55 # this is the buffer I set for the longest sent length. Based on the longest sentence before adding this in\n",
    "eng_seq_collection = []\n",
    "for sentence in en_sentences:\n",
    "    eng_seq_sentence = []\n",
    "    # Start and end each sequence with <s> and </s>\n",
    "    # This allows the model to know when each string stops and starts (when it should stop predicting the next token)\n",
    "    eng_seq_sentence.append(eng_word2index['<s>'])\n",
    "    for word in sentence.strip().split(' '):\n",
    "        eng_seq_sentence.append(eng_word2index[word])\n",
    "\n",
    "    eng_seq_sentence.append(eng_word2index['</s>'])\n",
    "    # # determine the max sentence length\n",
    "    # max_eng_sent_len = max(max_eng_sent_len, len(eng_seq_sentence))\n",
    "    # pad sentences to all be the same length\n",
    "    difference = max_eng_sent_len - len(eng_seq_sentence)\n",
    "    if difference > 0:\n",
    "        eng_seq_sentence.extend(repeat(eng_word2index[\"</pad>\"], difference))\n",
    "        \n",
    "    eng_seq_collection.append(eng_seq_sentence)\n",
    "    \n",
    "\n",
    "# gujarati word to index\n",
    "max_guj_sent_len = 50 # this is the buffer I set for the longest sent length. Based on the longest sentence length before adding this in\n",
    "guj_seq_collection = []\n",
    "for sentence in gu_sentences:\n",
    "    guj_seq_sentence = []\n",
    "    guj_seq_sentence.append(guj_word2index['<s>'])\n",
    "    for word in sentence.strip().split(' '):\n",
    "        # get rid of the zero-byte characters\n",
    "        try:\n",
    "            word = word.replace(\"\\u200b\",\"\")\n",
    "            guj_seq_sentence.append(guj_word2index[word])\n",
    "        except KeyError:\n",
    "            print(\"not working\", sentence)\n",
    "            raise KeyError\n",
    "    \n",
    "    guj_seq_sentence.append(guj_word2index['</s>'])\n",
    "\n",
    "    # pad sentences to all be the same length\n",
    "    difference = max_guj_sent_len - len(guj_seq_sentence)\n",
    "    if difference > 0:\n",
    "        guj_seq_sentence.extend(repeat(guj_word2index[\"</પેડ>\"], difference))\n",
    "    # # determine the max sentence length\n",
    "    # max_guj_sent_len = max(max_guj_sent_len, len(guj_seq_sentence))\n",
    "\n",
    "    guj_seq_collection.append(guj_seq_sentence)\n",
    "\n",
    "\n",
    "    # list of indexes for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "55\n",
      "55\n",
      "55\n",
      "55\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "# confirm that all the sentences are the same size\n",
    "for sentence in guj_seq_collection[0:5]:\n",
    "    print(len(sentence))\n",
    "\n",
    "for sentence in eng_seq_collection[0:5]:\n",
    "    print(len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "# This is code that I ran BEFORE adding in the sentences padding\n",
    "\n",
    "len_of_sent = []\n",
    "for i in guj_seq_collection:\n",
    "    len_of_sent.append(len(i))\n",
    "# Find longest sentence\n",
    "print(max(len_of_sent))\n",
    "\n",
    "# max seq len = 50\n",
    "\n",
    "\n",
    "len_of_sent = []\n",
    "for i in eng_seq_collection:\n",
    "    len_of_sent.append(len(i))\n",
    "# Find longest sentence\n",
    "print(max(len_of_sent))\n",
    "\n",
    "# max seq len = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([56])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/cqrdw531507g4cfv8ncd8fz80000gp/T/ipykernel_1216/2268196282.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoder_input = t.tensor(t.zeros(batch_size, sequence_length)).long()\n"
     ]
    }
   ],
   "source": [
    "# input tensor for embedding\n",
    "# [batch size, token length]\n",
    "# vocab size: 17472 (english)\n",
    "# token length (arbitarry) - 50 or 60 tokens\n",
    "\n",
    "# dummy encoder input\n",
    "batch_size = 10 # how should this be determined?\n",
    "eng_vocab_size = 17500 # based on vocab size above\n",
    "sequence_length = 56 # based on max sentence length (english sentencelength)\n",
    "\n",
    "encoder_input = t.tensor(t.zeros(batch_size, sequence_length)).long()\n",
    "\n",
    "# decoder output: [batch, seqlen, vocab]\n",
    "# to [batch, seqlen] (using argmax)\n",
    "# convert output to list: .list()\n",
    "# use dict to convert from indexes to words\n",
    "\n",
    "print(encoder_input[0])\n",
    "print(encoder_input[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, eng_vocab_size, hidden_size, num_layers, dropout_p=0.1):\n",
    "        \"\"\"\n",
    "        embedding_dim: for each word in vocab_size, this is the number of floats representing each word\n",
    "        vocab_size: the size of the original english vocab len(en_words)\n",
    "        hidden_size: the number of LSTM cells in each layer\n",
    "        num_layers: the number of LSTM layers in the model\n",
    "        dropout_p: \n",
    "        \"\"\"\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Step 1: Create the Embedding Matrix\n",
    "            # input dimensions: tensor[batch_size, sequence_length]\n",
    "            # output dimensions: tensor[batch_size, sentence_length, embedding_dim]\n",
    "            # creates the embedding matrix to be of dimensions vocab size x embedding_dim\n",
    "            # the actual values are random to begin with\n",
    "            # the embedding matrix is unique (lookup table, key is the index, value is the embedding for that word)\n",
    "        self.embedding = nn.Embedding(eng_vocab_size, embedding_dim)\n",
    "        # this just builds the LSTM architechture. Also random floats\n",
    "        # Step 2: Run the LSTM Model\n",
    "            # In: input, (h_n, c_n)\n",
    "            # input dimensions: (for batched) tensor[batch_size, sequence_length, input_size] AKA tensor[batch_size, sentence_length, embedding_dim]\n",
    "            # h_n input dimensions: tensor[#_layers, hidden_size] of zeros since we don't specify\n",
    "            # h_c input dimensions: tensor[#_layers, batch_size, hidden_size] of zeros since we don't specify\n",
    "            # Out: output, (h_n, c_n)\n",
    "            # output dimensions: (for batched) tensor[batch_size, sequence_length, hidden_size] AKA tensor[batch_size, sentence_length, number_of_LSTMcells]\n",
    "            # h_n output dimensions: this is the hidden output (short term memory). tensor[#_layers, batch_size, hidden_size]\n",
    "            # h_c output dimensions: this is the cell output (long term memory). tensor[#_layers, batch_size, hidden_size]\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout_p, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "# Create the encoder with all the parameters\n",
    "encoder = EncoderLSTM(embedding_dim=300, eng_vocab_size=eng_vocab_size, hidden_size=128, num_layers= 2, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output, encoder_output_hidden = encoder(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 56, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([2, 10, 128]) torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder_output_hidden))\n",
    "\n",
    "print(encoder_output_hidden[0].shape, encoder_output_hidden[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 56, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([10, 56, 128])\n",
      "h_n output shape: torch.Size([2, 10, 128])\n",
      "h_c output shape: torch.Size([2, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the output dimensions are what we expect:\n",
    "print(\"output shape:\", encoder_output.shape)\n",
    "print(\"h_n output shape:\", encoder_output_hidden[0].shape)\n",
    "print(\"h_c output shape:\", encoder_output_hidden[1].shape)\n",
    "\n",
    "# the h_c output (context vector) layer is what goes into the decoder\n",
    "\n",
    "# h_n is the last element in the output\n",
    "# the 12th element of output == h_n\n",
    "# but order is flipped (batch_first only on output, not on the hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dummy decoder input\n",
    "guj_vocab_size = 25500 # based on vocab size above\n",
    "# decoder_input = t.tensor(t.zeros(batch_size, guj_vocab_size)).long() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, guj_vocab_size, hidden_size, num_layers, dropout_p=0.1):\n",
    "        \"\"\"\n",
    "        embedding_dim: for each word in guj_vocab_size, this is the number of floats representing each word\n",
    "        guj_vocab_size: the size of the original gujarati vocab len(gu_words)\n",
    "        hidden_size: the number of LSTM cells in each layer\n",
    "        num_layers: the number of LSTM layers in the model\n",
    "        dropout_p: \n",
    "        \"\"\"\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        # Step 1: Create the Embedding Matrix\n",
    "        # input dimensions: tensor[batch_size, sequence_length]\n",
    "        # output dimensions: tensor[batch_size, sentence_length, embedding_dim]\n",
    "        # creates the embedding matrix to be of dimensions vocab size x embedding_dim\n",
    "        # the actual values are random to begin with\n",
    "        # the embedding matrix is unique (lookup table, key is the index, value is the embedding for that word)\n",
    "        self.embedding = nn.Embedding(guj_vocab_size, embedding_dim)\n",
    "        # Step 2: Run the LSTM Model\n",
    "        # In: input, (h_n, c_n)\n",
    "        # input dimensions: (for batched) tensor[batch_size, sequence_length, input_size] AKA tensor[batch_size, sentence_length, embedding_dim]\n",
    "        # h_n input dimensions: tensor[#_layers, hidden_size] of zeros since we don't specify\n",
    "        # h_c input dimensions: tensor[#_layers, batch_size, hidden_size] of zeros since we don't specify\n",
    "        # Out: output, (h_n, c_n)\n",
    "        # output dimensions: (for batched) tensor[batch_size, sequence_length, hidden_size] AKA tensor[batch_size, sentence_length, number_of_LSTMcells]\n",
    "        # h_n output dimensions: this is the hidden output (short term memory). tensor[#_layers, batch_size, hidden_size]\n",
    "        # h_c output dimensions: this is the cell output (long term memory). tensor[#_layers, batch_size, hidden_size]\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_p, batch_first=True)\n",
    "        # Step 3: Generate the predicted word. This will give us the next word, and will also be used as a hidden layer for the next prediction\n",
    "        # This is done by performing a linear transormation on \n",
    "        # In: any # of dimensions (including none), in_features\n",
    "        # Out\n",
    "        self.generator = nn.Linear(in_features=hidden_size, out_features=guj_vocab_size) \n",
    "\n",
    "    def forward(self, encoder_hidden: t.Tensor):\n",
    "        # Big picture: \n",
    "        # The decoder takes in the hidden layer from the encoder, as well as the previous predicted word\n",
    "        # For the first round, the previous predicted word is the start token\n",
    "        # encoder_hidden # of size [batch_size, sequence_length, d_model_encoder] \n",
    "        batch_size = encoder_hidden.shape[0] # take the first element (batch size) of the second hidden layer (context vector)\n",
    "        # why do we do this?? below\n",
    "        decoder_hidden = encoder_hidden # value from encoder\n",
    "        decoder_input_int = guj_word2index[\"<s>\"] #  Get the index of the start token. this value is 1\n",
    "        # goal is to get decoder_input to look like this\n",
    "        # decoder_input * batch_size [1, 1, 1, 1] (len of batch size)\n",
    "        # decoder_input should be a tensor of dimension [batch_size]\n",
    "        decoder_input = t.tensor(t.ones(batch_size)*decoder_input_int).long()\n",
    "        # forcing the batch size here so it must be 10. This is we why drop 1 seq\n",
    "\n",
    "        # can feed in multiple sentences at a time (output is a list of tensors)\n",
    "        decoder_outputs: list[t.Tensor] = [] # this is a list of 2D tensors. This is why we need stack later to force 3D\n",
    "        # Our longest gujarati sentence is 46 tokens, so we will predict the next word up tp 50 times. \n",
    "        # This limit is in place to prevent our model from running infinitely\n",
    "        for n in range(50):\n",
    "            output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            # output is batch size x vocab size\n",
    "            # but we just need the index of the token (like the original input)\n",
    "            # this is the predicted token that will be fed into the next step of the model run\n",
    "            # can do this with the argmax\n",
    "            # TODO: implement beam search \n",
    "            decoder_input = t.argmax(output, dim=-1).detach() # taking the max prob (explicitly greedy search!) argmax over vocab size!\n",
    "            # detach here so that we don't compute gradients on this decoder_input\n",
    "            decoder_outputs.append(output) # we want to save all of the predicted words we get along the way. Why??\n",
    "\n",
    "        decoder_outputs = t.stack(decoder_outputs, dim=1) #concatenate into tensor of tensors; [batch_size, seq_len, guj_vocab_size], adds another dimension\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1) # softmax all the tensors at one time, over the guj_vocab_size dimension\n",
    "        return decoder_outputs, decoder_hidden # We return `None` for consistency in the training loop????\n",
    "    \n",
    "    def forward_step(self, previous_output, hidden):\n",
    "        \"\"\"\n",
    "        Conducts a forward step through one LSTM cell in the decoder.\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor[batch_size, guj_vocab_size])\n",
    "            hidden (Tensor[batch_size, hidden_size])\n",
    "        \"\"\"\n",
    "\n",
    "        output = self.embedding(previous_output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        # output = F.relu(output)\n",
    "        output = self.generator(output)\n",
    "        return output, hidden\n",
    "    \n",
    "# Create the decoder with all the parameters\n",
    "decoder = DecoderLSTM(embedding_dim=300, guj_vocab_size=guj_vocab_size, hidden_size=128, num_layers= 2, dropout_p=0.1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times do I want to do a forward step (how many words do I want to predict?)\n",
    "# 50 (len of longest sentence is 46 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion):\n",
    "    '''\n",
    "    '''\n",
    "    input_tensor, target_tensor = batch\n",
    "    encoder_output, encoder_output_hidden = encoder(input_tensor)\n",
    "    encoder_out = t.transpose(encoder_output_hidden[1], 0, 1)\n",
    "    output, hidden = decoder(encoder_out)\n",
    "    loss = criterion(\n",
    "        output.view(-1, output.size(-1)), target_tensor.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(batch, encoder, decoder):\n",
    "        '''\n",
    "        Calculates & returns the accuracy on the tokens in the batch (i.e. how often the model's prediction\n",
    "        is correct). Logging should happen in the `train` function (after we've computed the accuracy for\n",
    "        the whole validation set).\n",
    "        '''\n",
    "        input_tensor, target_tensor = batch\n",
    "        encoder_output, encoder_output_hidden = encoder(input_tensor)\n",
    "        encoder_out = t.transpose(encoder_output_hidden[1], 0, 1)\n",
    "        output, hidden = decoder(encoder_out)\n",
    "\n",
    "        predictions = output.argmax(dim=-1)\n",
    "        correct = (predictions == target_tensor).flatten()\n",
    "        return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "def train(train_dataloader, val_dataloader, encoder, decoder, n_epochs, learning_rate=0.001):\n",
    "        # #print_every=100, plot_every=100):\n",
    "        \n",
    "        # # my old code\n",
    "        # start = time.time()\n",
    "        # plot_losses = []\n",
    "        # print_loss_total = 0  # Reset every print_every\n",
    "        # plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "        # Fern's plot code\n",
    "        accuracy = np.nan\n",
    "\n",
    "        progress_bar = tqdm(total = len(train_dataloader) * n_epochs)\n",
    "\n",
    "        loss_logs = []\n",
    "        loss_fig = plt.figure()\n",
    "        loss_ax = plt.gca()\n",
    "        loss_display = display(loss_fig, display_id=True)\n",
    "        \n",
    "        acc_logs = []\n",
    "        acc_fig = plt.figure()\n",
    "        acc_ax = plt.gca()\n",
    "        acc_display = display(acc_fig, display_id=True) \n",
    "\n",
    "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "        criterion = nn.NLLLoss() #negative log likelihood loss\n",
    "\n",
    "        for epoch in range(0, n_epochs):\n",
    "            step = 0\n",
    "            for i, batch in enumerate(train_dataloader):\n",
    "                step += 1\n",
    "                loss = training_step(batch, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion)\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.2f}\")\n",
    "\n",
    "                loss_logs.append(np.array(object=loss.detach().to(device)))\n",
    "            \n",
    "                if step % 100 == 0:\n",
    "                    loss_ax.clear()\n",
    "                    loss_ax.plot(loss_logs[100:])\n",
    "                    loss_fig.canvas.draw()\n",
    "                    loss_display.update(obj=loss_fig)\n",
    "\n",
    "            correct_predictions = t.concat([validation_step(batch, encoder, decoder) for batch in val_dataloader])\n",
    "            accuracy = correct_predictions.detach().to(device).float().mean().item()\n",
    "\n",
    "            acc_logs.append(np.array(object=accuracy))\n",
    "            acc_ax.clear()\n",
    "            acc_ax.plot(acc_logs)\n",
    "            acc_fig.canvas.draw()\n",
    "            acc_display.update(obj=acc_fig)\n",
    "\n",
    "            # proj_directory = \"/Users/mkshah605/Documents/GitHub/RC/NMT_IndicLang/\"\n",
    "\n",
    "            # loss_fig.savefig(proj_directory + \"images/loss_fig.png\")\n",
    "            # acc_fig.savefig(proj_directory + \"images/acc_fig.png\")\n",
    "        \n",
    "        return loss_logs, acc_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4550 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/cqrdw531507g4cfv8ncd8fz80000gp/T/ipykernel_1216/2293439555.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = t.tensor(t.ones(batch_size)*decoder_input_int).long()\n",
      "Epoch 1, loss: 7.174, accuracy: nan: 100%|██████████| 4550/4550 [1:08:10<00:00,  1.11it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array(10.170965, dtype=float32),\n",
       "  array(10.12511, dtype=float32),\n",
       "  array(10.07653, dtype=float32),\n",
       "  array(10.047723, dtype=float32),\n",
       "  array(10.020679, dtype=float32),\n",
       "  array(9.922576, dtype=float32),\n",
       "  array(9.743904, dtype=float32),\n",
       "  array(9.507356, dtype=float32),\n",
       "  array(9.214358, dtype=float32),\n",
       "  array(8.903859, dtype=float32),\n",
       "  array(8.560115, dtype=float32),\n",
       "  array(8.128089, dtype=float32),\n",
       "  array(7.7504134, dtype=float32),\n",
       "  array(7.360582, dtype=float32),\n",
       "  array(7.0671844, dtype=float32),\n",
       "  array(6.645515, dtype=float32),\n",
       "  array(6.246201, dtype=float32),\n",
       "  array(5.878709, dtype=float32),\n",
       "  array(5.5537934, dtype=float32),\n",
       "  array(5.211566, dtype=float32),\n",
       "  array(4.9281425, dtype=float32),\n",
       "  array(4.720228, dtype=float32),\n",
       "  array(4.291346, dtype=float32),\n",
       "  array(3.9300354, dtype=float32),\n",
       "  array(3.9452662, dtype=float32),\n",
       "  array(3.3112128, dtype=float32),\n",
       "  array(3.2309163, dtype=float32),\n",
       "  array(2.927772, dtype=float32),\n",
       "  array(2.720133, dtype=float32),\n",
       "  array(2.5859227, dtype=float32),\n",
       "  array(2.3926735, dtype=float32),\n",
       "  array(2.065933, dtype=float32),\n",
       "  array(2.045815, dtype=float32),\n",
       "  array(2.0945275, dtype=float32),\n",
       "  array(2.0935934, dtype=float32),\n",
       "  array(1.9337443, dtype=float32),\n",
       "  array(1.989075, dtype=float32),\n",
       "  array(2.0266125, dtype=float32),\n",
       "  array(1.7727427, dtype=float32),\n",
       "  array(1.7456852, dtype=float32),\n",
       "  array(1.6617284, dtype=float32),\n",
       "  array(1.8430346, dtype=float32),\n",
       "  array(1.9252753, dtype=float32),\n",
       "  array(1.7450191, dtype=float32),\n",
       "  array(1.6348599, dtype=float32),\n",
       "  array(1.9039187, dtype=float32),\n",
       "  array(1.822805, dtype=float32),\n",
       "  array(2.2319815, dtype=float32),\n",
       "  array(1.9934021, dtype=float32),\n",
       "  array(1.9301647, dtype=float32),\n",
       "  array(1.6642132, dtype=float32),\n",
       "  array(1.751717, dtype=float32),\n",
       "  array(1.6914839, dtype=float32),\n",
       "  array(2.3152332, dtype=float32),\n",
       "  array(1.8090258, dtype=float32),\n",
       "  array(1.7891601, dtype=float32),\n",
       "  array(1.9226513, dtype=float32),\n",
       "  array(1.9739184, dtype=float32),\n",
       "  array(2.1485476, dtype=float32),\n",
       "  array(1.9042996, dtype=float32),\n",
       "  array(1.9057806, dtype=float32),\n",
       "  array(1.9377404, dtype=float32),\n",
       "  array(1.8176885, dtype=float32),\n",
       "  array(1.908118, dtype=float32),\n",
       "  array(2.24676, dtype=float32),\n",
       "  array(2.1605814, dtype=float32),\n",
       "  array(2.0207875, dtype=float32),\n",
       "  array(2.0476627, dtype=float32),\n",
       "  array(1.9037557, dtype=float32),\n",
       "  array(2.068803, dtype=float32),\n",
       "  array(2.1426935, dtype=float32),\n",
       "  array(2.2912574, dtype=float32),\n",
       "  array(2.4186184, dtype=float32),\n",
       "  array(2.1755407, dtype=float32),\n",
       "  array(2.1211264, dtype=float32),\n",
       "  array(1.8080549, dtype=float32),\n",
       "  array(1.9922221, dtype=float32),\n",
       "  array(2.0077507, dtype=float32),\n",
       "  array(2.4509416, dtype=float32),\n",
       "  array(2.1831496, dtype=float32),\n",
       "  array(1.929113, dtype=float32),\n",
       "  array(2.1248398, dtype=float32),\n",
       "  array(2.2230623, dtype=float32),\n",
       "  array(2.155398, dtype=float32),\n",
       "  array(2.2392042, dtype=float32),\n",
       "  array(2.521076, dtype=float32),\n",
       "  array(2.5190601, dtype=float32),\n",
       "  array(2.034374, dtype=float32),\n",
       "  array(2.1686208, dtype=float32),\n",
       "  array(2.700248, dtype=float32),\n",
       "  array(2.950297, dtype=float32),\n",
       "  array(2.4581122, dtype=float32),\n",
       "  array(3.153845, dtype=float32),\n",
       "  array(2.50294, dtype=float32),\n",
       "  array(2.3962443, dtype=float32),\n",
       "  array(2.276956, dtype=float32),\n",
       "  array(2.1661282, dtype=float32),\n",
       "  array(2.6347063, dtype=float32),\n",
       "  array(1.8244219, dtype=float32),\n",
       "  array(2.4426153, dtype=float32),\n",
       "  array(2.9576433, dtype=float32),\n",
       "  array(2.485085, dtype=float32),\n",
       "  array(2.4471457, dtype=float32),\n",
       "  array(2.1077719, dtype=float32),\n",
       "  array(3.4436932, dtype=float32),\n",
       "  array(2.5899441, dtype=float32),\n",
       "  array(2.3281577, dtype=float32),\n",
       "  array(2.206574, dtype=float32),\n",
       "  array(2.3410542, dtype=float32),\n",
       "  array(2.797372, dtype=float32),\n",
       "  array(2.228557, dtype=float32),\n",
       "  array(2.1131968, dtype=float32),\n",
       "  array(2.1852634, dtype=float32),\n",
       "  array(2.4336483, dtype=float32),\n",
       "  array(3.2858198, dtype=float32),\n",
       "  array(2.6074965, dtype=float32),\n",
       "  array(2.1417933, dtype=float32),\n",
       "  array(4.0213747, dtype=float32),\n",
       "  array(2.3213203, dtype=float32),\n",
       "  array(2.3066216, dtype=float32),\n",
       "  array(2.4718642, dtype=float32),\n",
       "  array(2.59024, dtype=float32),\n",
       "  array(2.1169136, dtype=float32),\n",
       "  array(2.6879263, dtype=float32),\n",
       "  array(2.3826706, dtype=float32),\n",
       "  array(2.7659564, dtype=float32),\n",
       "  array(2.7087812, dtype=float32),\n",
       "  array(2.1857524, dtype=float32),\n",
       "  array(2.562723, dtype=float32),\n",
       "  array(2.6261275, dtype=float32),\n",
       "  array(2.8170543, dtype=float32),\n",
       "  array(2.2879248, dtype=float32),\n",
       "  array(2.474734, dtype=float32),\n",
       "  array(2.6319058, dtype=float32),\n",
       "  array(2.5496383, dtype=float32),\n",
       "  array(2.5687065, dtype=float32),\n",
       "  array(2.5998569, dtype=float32),\n",
       "  array(2.9946861, dtype=float32),\n",
       "  array(2.9754605, dtype=float32),\n",
       "  array(2.3057008, dtype=float32),\n",
       "  array(2.1713428, dtype=float32),\n",
       "  array(3.065126, dtype=float32),\n",
       "  array(2.3710477, dtype=float32),\n",
       "  array(2.2122333, dtype=float32),\n",
       "  array(2.8902888, dtype=float32),\n",
       "  array(2.3626196, dtype=float32),\n",
       "  array(2.9381082, dtype=float32),\n",
       "  array(2.8127916, dtype=float32),\n",
       "  array(2.8722522, dtype=float32),\n",
       "  array(2.6160765, dtype=float32),\n",
       "  array(2.3968222, dtype=float32),\n",
       "  array(2.6885195, dtype=float32),\n",
       "  array(2.6573813, dtype=float32),\n",
       "  array(2.249297, dtype=float32),\n",
       "  array(2.7321281, dtype=float32),\n",
       "  array(2.4553084, dtype=float32),\n",
       "  array(1.9895589, dtype=float32),\n",
       "  array(3.059144, dtype=float32),\n",
       "  array(2.4142313, dtype=float32),\n",
       "  array(2.4967968, dtype=float32),\n",
       "  array(2.778222, dtype=float32),\n",
       "  array(2.9197624, dtype=float32),\n",
       "  array(2.793492, dtype=float32),\n",
       "  array(2.932956, dtype=float32),\n",
       "  array(2.7885756, dtype=float32),\n",
       "  array(2.5114877, dtype=float32),\n",
       "  array(2.5514712, dtype=float32),\n",
       "  array(2.882807, dtype=float32),\n",
       "  array(2.7083392, dtype=float32),\n",
       "  array(2.5068245, dtype=float32),\n",
       "  array(2.5337033, dtype=float32),\n",
       "  array(2.438212, dtype=float32),\n",
       "  array(2.6031463, dtype=float32),\n",
       "  array(3.6507096, dtype=float32),\n",
       "  array(2.7946212, dtype=float32),\n",
       "  array(3.9088032, dtype=float32),\n",
       "  array(3.0835066, dtype=float32),\n",
       "  array(3.3301513, dtype=float32),\n",
       "  array(2.9067724, dtype=float32),\n",
       "  array(2.9682822, dtype=float32),\n",
       "  array(2.6495326, dtype=float32),\n",
       "  array(3.4688838, dtype=float32),\n",
       "  array(3.3296912, dtype=float32),\n",
       "  array(2.5354302, dtype=float32),\n",
       "  array(2.777688, dtype=float32),\n",
       "  array(3.1063857, dtype=float32),\n",
       "  array(2.9136035, dtype=float32),\n",
       "  array(2.9898176, dtype=float32),\n",
       "  array(2.775798, dtype=float32),\n",
       "  array(3.2431622, dtype=float32),\n",
       "  array(2.7320483, dtype=float32),\n",
       "  array(2.9142306, dtype=float32),\n",
       "  array(3.2740617, dtype=float32),\n",
       "  array(3.0244615, dtype=float32),\n",
       "  array(2.785168, dtype=float32),\n",
       "  array(3.0320134, dtype=float32),\n",
       "  array(3.826077, dtype=float32),\n",
       "  array(2.8425057, dtype=float32),\n",
       "  array(2.413741, dtype=float32),\n",
       "  array(2.9774714, dtype=float32),\n",
       "  array(3.1303082, dtype=float32),\n",
       "  array(2.978282, dtype=float32),\n",
       "  array(2.6568804, dtype=float32),\n",
       "  array(3.0629504, dtype=float32),\n",
       "  array(2.9256258, dtype=float32),\n",
       "  array(2.861346, dtype=float32),\n",
       "  array(3.1483564, dtype=float32),\n",
       "  array(3.1936693, dtype=float32),\n",
       "  array(2.816329, dtype=float32),\n",
       "  array(3.2576928, dtype=float32),\n",
       "  array(3.770468, dtype=float32),\n",
       "  array(3.5357122, dtype=float32),\n",
       "  array(3.3683584, dtype=float32),\n",
       "  array(3.3137228, dtype=float32),\n",
       "  array(3.443798, dtype=float32),\n",
       "  array(3.2406995, dtype=float32),\n",
       "  array(3.3050373, dtype=float32),\n",
       "  array(2.8193157, dtype=float32),\n",
       "  array(3.433191, dtype=float32),\n",
       "  array(3.010019, dtype=float32),\n",
       "  array(3.0612638, dtype=float32),\n",
       "  array(3.3152423, dtype=float32),\n",
       "  array(2.9881678, dtype=float32),\n",
       "  array(3.025529, dtype=float32),\n",
       "  array(3.1043043, dtype=float32),\n",
       "  array(3.7631454, dtype=float32),\n",
       "  array(3.8381572, dtype=float32),\n",
       "  array(2.7272468, dtype=float32),\n",
       "  array(3.350911, dtype=float32),\n",
       "  array(3.7144926, dtype=float32),\n",
       "  array(3.600126, dtype=float32),\n",
       "  array(3.2834144, dtype=float32),\n",
       "  array(3.1426795, dtype=float32),\n",
       "  array(3.1294656, dtype=float32),\n",
       "  array(3.19504, dtype=float32),\n",
       "  array(4.0600953, dtype=float32),\n",
       "  array(3.55106, dtype=float32),\n",
       "  array(3.604798, dtype=float32),\n",
       "  array(2.943514, dtype=float32),\n",
       "  array(3.9772563, dtype=float32),\n",
       "  array(4.219595, dtype=float32),\n",
       "  array(2.9684324, dtype=float32),\n",
       "  array(3.1320333, dtype=float32),\n",
       "  array(4.193436, dtype=float32),\n",
       "  array(3.142718, dtype=float32),\n",
       "  array(3.490993, dtype=float32),\n",
       "  array(3.071905, dtype=float32),\n",
       "  array(3.7238028, dtype=float32),\n",
       "  array(3.4435809, dtype=float32),\n",
       "  array(3.289161, dtype=float32),\n",
       "  array(3.6662097, dtype=float32),\n",
       "  array(4.0840225, dtype=float32),\n",
       "  array(3.908029, dtype=float32),\n",
       "  array(3.5886073, dtype=float32),\n",
       "  array(3.6680682, dtype=float32),\n",
       "  array(3.2058175, dtype=float32),\n",
       "  array(3.3804688, dtype=float32),\n",
       "  array(3.4046044, dtype=float32),\n",
       "  array(3.7893622, dtype=float32),\n",
       "  array(3.0302255, dtype=float32),\n",
       "  array(3.4313996, dtype=float32),\n",
       "  array(4.2709894, dtype=float32),\n",
       "  array(3.5852613, dtype=float32),\n",
       "  array(3.4212627, dtype=float32),\n",
       "  array(3.4316378, dtype=float32),\n",
       "  array(2.5752125, dtype=float32),\n",
       "  array(3.163315, dtype=float32),\n",
       "  array(3.875301, dtype=float32),\n",
       "  array(3.6924348, dtype=float32),\n",
       "  array(4.0025477, dtype=float32),\n",
       "  array(3.3651125, dtype=float32),\n",
       "  array(4.3096285, dtype=float32),\n",
       "  array(3.5086055, dtype=float32),\n",
       "  array(4.0058293, dtype=float32),\n",
       "  array(3.4676414, dtype=float32),\n",
       "  array(3.715542, dtype=float32),\n",
       "  array(3.049259, dtype=float32),\n",
       "  array(3.2076058, dtype=float32),\n",
       "  array(4.7507267, dtype=float32),\n",
       "  array(3.591324, dtype=float32),\n",
       "  array(3.6243105, dtype=float32),\n",
       "  array(3.7057714, dtype=float32),\n",
       "  array(4.0387225, dtype=float32),\n",
       "  array(3.767401, dtype=float32),\n",
       "  array(4.322161, dtype=float32),\n",
       "  array(3.9582145, dtype=float32),\n",
       "  array(4.563394, dtype=float32),\n",
       "  array(3.8547416, dtype=float32),\n",
       "  array(3.3058958, dtype=float32),\n",
       "  array(3.6194503, dtype=float32),\n",
       "  array(3.8567474, dtype=float32),\n",
       "  array(3.7090003, dtype=float32),\n",
       "  array(4.279503, dtype=float32),\n",
       "  array(4.203428, dtype=float32),\n",
       "  array(3.900132, dtype=float32),\n",
       "  array(4.6715493, dtype=float32),\n",
       "  array(3.6642087, dtype=float32),\n",
       "  array(4.4417615, dtype=float32),\n",
       "  array(4.3322763, dtype=float32),\n",
       "  array(3.5078073, dtype=float32),\n",
       "  array(4.4958377, dtype=float32),\n",
       "  array(3.7677042, dtype=float32),\n",
       "  array(5.0579753, dtype=float32),\n",
       "  array(4.6281214, dtype=float32),\n",
       "  array(3.4566565, dtype=float32),\n",
       "  array(4.2409077, dtype=float32),\n",
       "  array(4.96556, dtype=float32),\n",
       "  array(3.6092038, dtype=float32),\n",
       "  array(4.1037827, dtype=float32),\n",
       "  array(4.968981, dtype=float32),\n",
       "  array(4.072068, dtype=float32),\n",
       "  array(3.609585, dtype=float32),\n",
       "  array(4.1319966, dtype=float32),\n",
       "  array(3.5282393, dtype=float32),\n",
       "  array(3.7788, dtype=float32),\n",
       "  array(4.158168, dtype=float32),\n",
       "  array(4.574616, dtype=float32),\n",
       "  array(3.761521, dtype=float32),\n",
       "  array(4.0904837, dtype=float32),\n",
       "  array(4.3312564, dtype=float32),\n",
       "  array(4.2734494, dtype=float32),\n",
       "  array(4.503505, dtype=float32),\n",
       "  array(4.3535976, dtype=float32),\n",
       "  array(4.7020655, dtype=float32),\n",
       "  array(4.41506, dtype=float32),\n",
       "  array(4.601875, dtype=float32),\n",
       "  array(3.8660924, dtype=float32),\n",
       "  array(3.9576864, dtype=float32),\n",
       "  array(4.6595674, dtype=float32),\n",
       "  array(3.5595617, dtype=float32),\n",
       "  array(4.522988, dtype=float32),\n",
       "  array(4.5571737, dtype=float32),\n",
       "  array(3.4889853, dtype=float32),\n",
       "  array(3.9022903, dtype=float32),\n",
       "  array(4.579693, dtype=float32),\n",
       "  array(4.3418255, dtype=float32),\n",
       "  array(5.082462, dtype=float32),\n",
       "  array(4.448161, dtype=float32),\n",
       "  array(3.972124, dtype=float32),\n",
       "  array(3.6621602, dtype=float32),\n",
       "  array(3.9096236, dtype=float32),\n",
       "  array(4.823369, dtype=float32),\n",
       "  array(3.4807193, dtype=float32),\n",
       "  array(4.373189, dtype=float32),\n",
       "  array(4.4632983, dtype=float32),\n",
       "  array(4.412298, dtype=float32),\n",
       "  array(3.8933983, dtype=float32),\n",
       "  array(4.022895, dtype=float32),\n",
       "  array(4.8193555, dtype=float32),\n",
       "  array(5.193456, dtype=float32),\n",
       "  array(3.7712066, dtype=float32),\n",
       "  array(5.124802, dtype=float32),\n",
       "  array(5.2652884, dtype=float32),\n",
       "  array(4.2462277, dtype=float32),\n",
       "  array(3.4267201, dtype=float32),\n",
       "  array(4.3739676, dtype=float32),\n",
       "  array(4.720809, dtype=float32),\n",
       "  array(4.7418027, dtype=float32),\n",
       "  array(4.132444, dtype=float32),\n",
       "  array(4.7974887, dtype=float32),\n",
       "  array(4.2507524, dtype=float32),\n",
       "  array(5.0858006, dtype=float32),\n",
       "  array(3.9125276, dtype=float32),\n",
       "  array(3.8700993, dtype=float32),\n",
       "  array(4.4246383, dtype=float32),\n",
       "  array(4.497318, dtype=float32),\n",
       "  array(3.8363557, dtype=float32),\n",
       "  array(3.8490257, dtype=float32),\n",
       "  array(4.7574077, dtype=float32),\n",
       "  array(3.8136916, dtype=float32),\n",
       "  array(4.23549, dtype=float32),\n",
       "  array(4.2275124, dtype=float32),\n",
       "  array(3.418413, dtype=float32),\n",
       "  array(5.2932262, dtype=float32),\n",
       "  array(3.901551, dtype=float32),\n",
       "  array(3.8481457, dtype=float32),\n",
       "  array(5.326129, dtype=float32),\n",
       "  array(4.2687354, dtype=float32),\n",
       "  array(6.346121, dtype=float32),\n",
       "  array(4.5207386, dtype=float32),\n",
       "  array(5.2798905, dtype=float32),\n",
       "  array(4.5846643, dtype=float32),\n",
       "  array(5.326064, dtype=float32),\n",
       "  array(4.5936794, dtype=float32),\n",
       "  array(4.276024, dtype=float32),\n",
       "  array(4.2476788, dtype=float32),\n",
       "  array(8.201955, dtype=float32),\n",
       "  array(3.393295, dtype=float32),\n",
       "  array(4.6959276, dtype=float32),\n",
       "  array(3.7922158, dtype=float32),\n",
       "  array(5.4549847, dtype=float32),\n",
       "  array(5.4794445, dtype=float32),\n",
       "  array(4.593644, dtype=float32),\n",
       "  array(5.3416953, dtype=float32),\n",
       "  array(4.329466, dtype=float32),\n",
       "  array(4.996211, dtype=float32),\n",
       "  array(5.0239854, dtype=float32),\n",
       "  array(4.017047, dtype=float32),\n",
       "  array(4.4493966, dtype=float32),\n",
       "  array(4.9233627, dtype=float32),\n",
       "  array(5.353075, dtype=float32),\n",
       "  array(4.392792, dtype=float32),\n",
       "  array(4.0676827, dtype=float32),\n",
       "  array(6.0341306, dtype=float32),\n",
       "  array(4.2397385, dtype=float32),\n",
       "  array(4.401242, dtype=float32),\n",
       "  array(4.787712, dtype=float32),\n",
       "  array(4.3772306, dtype=float32),\n",
       "  array(4.7951508, dtype=float32),\n",
       "  array(6.118999, dtype=float32),\n",
       "  array(3.9853976, dtype=float32),\n",
       "  array(4.5343227, dtype=float32),\n",
       "  array(4.3493896, dtype=float32),\n",
       "  array(4.5957537, dtype=float32),\n",
       "  array(5.184711, dtype=float32),\n",
       "  array(5.3046403, dtype=float32),\n",
       "  array(4.333491, dtype=float32),\n",
       "  array(4.7493267, dtype=float32),\n",
       "  array(4.2316976, dtype=float32),\n",
       "  array(4.952357, dtype=float32),\n",
       "  array(5.9032817, dtype=float32),\n",
       "  array(4.0362463, dtype=float32),\n",
       "  array(5.0289264, dtype=float32),\n",
       "  array(7.3961854, dtype=float32),\n",
       "  array(4.1852336, dtype=float32),\n",
       "  array(5.710068, dtype=float32),\n",
       "  array(5.8220057, dtype=float32),\n",
       "  array(5.1718245, dtype=float32),\n",
       "  array(5.5406513, dtype=float32),\n",
       "  array(4.351714, dtype=float32),\n",
       "  array(4.1746483, dtype=float32),\n",
       "  array(5.421994, dtype=float32),\n",
       "  array(4.9919486, dtype=float32),\n",
       "  array(4.5131617, dtype=float32),\n",
       "  array(4.5767803, dtype=float32),\n",
       "  array(5.322564, dtype=float32),\n",
       "  array(4.8759646, dtype=float32),\n",
       "  array(5.9367423, dtype=float32),\n",
       "  array(5.0675373, dtype=float32),\n",
       "  array(5.5405817, dtype=float32),\n",
       "  array(5.251624, dtype=float32),\n",
       "  array(5.9181194, dtype=float32),\n",
       "  array(5.3072295, dtype=float32),\n",
       "  array(5.9999404, dtype=float32),\n",
       "  array(5.7688713, dtype=float32),\n",
       "  array(4.9122596, dtype=float32),\n",
       "  array(4.321746, dtype=float32),\n",
       "  array(5.1772966, dtype=float32),\n",
       "  array(6.2512803, dtype=float32),\n",
       "  array(5.9336247, dtype=float32),\n",
       "  array(4.899911, dtype=float32),\n",
       "  array(5.9708867, dtype=float32),\n",
       "  array(4.90509, dtype=float32),\n",
       "  array(5.466505, dtype=float32),\n",
       "  array(5.5027533, dtype=float32),\n",
       "  array(7.021246, dtype=float32),\n",
       "  array(6.782726, dtype=float32),\n",
       "  array(5.203761, dtype=float32),\n",
       "  array(5.703285, dtype=float32),\n",
       "  array(4.653266, dtype=float32),\n",
       "  array(5.335243, dtype=float32),\n",
       "  array(5.5780516, dtype=float32),\n",
       "  array(5.8372536, dtype=float32),\n",
       "  array(5.452934, dtype=float32),\n",
       "  array(5.9184647, dtype=float32),\n",
       "  array(5.226719, dtype=float32),\n",
       "  array(4.9444866, dtype=float32),\n",
       "  array(4.602034, dtype=float32),\n",
       "  array(5.6320662, dtype=float32),\n",
       "  array(5.508155, dtype=float32),\n",
       "  array(5.337003, dtype=float32),\n",
       "  array(5.8218765, dtype=float32),\n",
       "  array(5.7378044, dtype=float32),\n",
       "  array(5.9288573, dtype=float32),\n",
       "  array(5.097125, dtype=float32),\n",
       "  array(6.2095637, dtype=float32),\n",
       "  array(7.5043573, dtype=float32),\n",
       "  array(6.02471, dtype=float32),\n",
       "  array(5.411838, dtype=float32),\n",
       "  array(6.373965, dtype=float32),\n",
       "  array(6.6802998, dtype=float32),\n",
       "  array(5.2952127, dtype=float32),\n",
       "  array(5.295494, dtype=float32),\n",
       "  array(5.485899, dtype=float32),\n",
       "  array(5.0704403, dtype=float32),\n",
       "  array(5.608343, dtype=float32),\n",
       "  array(5.4933, dtype=float32),\n",
       "  array(6.50113, dtype=float32),\n",
       "  array(4.9540114, dtype=float32),\n",
       "  array(5.180951, dtype=float32),\n",
       "  array(5.5481796, dtype=float32),\n",
       "  array(7.707915, dtype=float32),\n",
       "  array(5.4876857, dtype=float32),\n",
       "  array(5.646536, dtype=float32),\n",
       "  array(5.398944, dtype=float32),\n",
       "  array(5.9273643, dtype=float32),\n",
       "  array(5.504177, dtype=float32),\n",
       "  array(5.933788, dtype=float32),\n",
       "  array(5.25248, dtype=float32),\n",
       "  array(5.3908763, dtype=float32),\n",
       "  array(6.02257, dtype=float32),\n",
       "  array(5.0216637, dtype=float32),\n",
       "  array(6.3976345, dtype=float32),\n",
       "  array(6.1108694, dtype=float32),\n",
       "  array(5.975496, dtype=float32),\n",
       "  array(7.068658, dtype=float32),\n",
       "  array(6.590654, dtype=float32),\n",
       "  array(6.124793, dtype=float32),\n",
       "  array(7.1483946, dtype=float32),\n",
       "  array(5.03147, dtype=float32),\n",
       "  array(6.3766465, dtype=float32),\n",
       "  array(6.627382, dtype=float32),\n",
       "  array(6.2520037, dtype=float32),\n",
       "  array(5.4268966, dtype=float32),\n",
       "  array(6.0221376, dtype=float32),\n",
       "  array(6.491159, dtype=float32),\n",
       "  array(6.0099077, dtype=float32),\n",
       "  array(7.420755, dtype=float32),\n",
       "  array(7.2098727, dtype=float32),\n",
       "  array(7.771564, dtype=float32),\n",
       "  array(6.183914, dtype=float32),\n",
       "  array(7.5139666, dtype=float32),\n",
       "  array(7.873536, dtype=float32),\n",
       "  array(6.0695853, dtype=float32),\n",
       "  array(5.615914, dtype=float32),\n",
       "  array(7.42952, dtype=float32),\n",
       "  array(6.0225534, dtype=float32),\n",
       "  array(8.147312, dtype=float32),\n",
       "  array(5.87152, dtype=float32),\n",
       "  array(7.5008807, dtype=float32),\n",
       "  array(6.3045416, dtype=float32),\n",
       "  array(6.633253, dtype=float32),\n",
       "  array(5.9380584, dtype=float32),\n",
       "  array(5.6348195, dtype=float32),\n",
       "  array(7.1822567, dtype=float32),\n",
       "  array(7.0985937, dtype=float32),\n",
       "  array(6.216527, dtype=float32),\n",
       "  array(6.7895384, dtype=float32),\n",
       "  array(6.437643, dtype=float32),\n",
       "  array(6.53483, dtype=float32),\n",
       "  array(7.9236894, dtype=float32),\n",
       "  array(5.2915683, dtype=float32),\n",
       "  array(7.262877, dtype=float32),\n",
       "  array(6.362251, dtype=float32),\n",
       "  array(7.542314, dtype=float32),\n",
       "  array(7.60386, dtype=float32),\n",
       "  array(6.221046, dtype=float32),\n",
       "  array(6.515485, dtype=float32),\n",
       "  array(5.0229774, dtype=float32),\n",
       "  array(6.434354, dtype=float32),\n",
       "  array(7.9906745, dtype=float32),\n",
       "  array(6.7147274, dtype=float32),\n",
       "  array(8.697912, dtype=float32),\n",
       "  array(5.574853, dtype=float32),\n",
       "  array(8.253412, dtype=float32),\n",
       "  array(6.2711205, dtype=float32),\n",
       "  array(6.0024667, dtype=float32),\n",
       "  array(6.051035, dtype=float32),\n",
       "  array(5.1755958, dtype=float32),\n",
       "  array(6.3921885, dtype=float32),\n",
       "  array(5.9669747, dtype=float32),\n",
       "  array(5.4851694, dtype=float32),\n",
       "  array(8.674602, dtype=float32),\n",
       "  array(6.719162, dtype=float32),\n",
       "  array(5.879228, dtype=float32),\n",
       "  array(7.3188224, dtype=float32),\n",
       "  array(5.8833385, dtype=float32),\n",
       "  array(5.816724, dtype=float32),\n",
       "  array(7.0013623, dtype=float32),\n",
       "  array(6.8398523, dtype=float32),\n",
       "  array(7.291112, dtype=float32),\n",
       "  array(7.7248454, dtype=float32),\n",
       "  array(6.862786, dtype=float32),\n",
       "  array(6.1892204, dtype=float32),\n",
       "  array(6.3788867, dtype=float32),\n",
       "  array(6.984681, dtype=float32),\n",
       "  array(5.440404, dtype=float32),\n",
       "  array(6.676668, dtype=float32),\n",
       "  array(5.828763, dtype=float32),\n",
       "  array(7.520303, dtype=float32),\n",
       "  array(7.5826483, dtype=float32),\n",
       "  array(7.6875615, dtype=float32),\n",
       "  array(6.6113434, dtype=float32),\n",
       "  array(6.5535674, dtype=float32),\n",
       "  array(6.556147, dtype=float32),\n",
       "  array(7.6295104, dtype=float32),\n",
       "  array(6.798824, dtype=float32),\n",
       "  array(7.025807, dtype=float32),\n",
       "  array(6.4466853, dtype=float32),\n",
       "  array(7.6896095, dtype=float32),\n",
       "  array(7.9964347, dtype=float32),\n",
       "  array(6.8902617, dtype=float32),\n",
       "  array(7.1714406, dtype=float32),\n",
       "  array(7.0458612, dtype=float32),\n",
       "  array(6.8614435, dtype=float32),\n",
       "  array(8.313892, dtype=float32),\n",
       "  array(6.7282443, dtype=float32),\n",
       "  array(6.4690056, dtype=float32),\n",
       "  array(6.934616, dtype=float32),\n",
       "  array(8.144227, dtype=float32),\n",
       "  array(7.6461873, dtype=float32),\n",
       "  array(7.2484336, dtype=float32),\n",
       "  array(7.392411, dtype=float32),\n",
       "  array(7.696039, dtype=float32),\n",
       "  array(6.610665, dtype=float32),\n",
       "  array(8.116864, dtype=float32),\n",
       "  array(7.274157, dtype=float32),\n",
       "  array(6.4423923, dtype=float32),\n",
       "  array(7.4416466, dtype=float32),\n",
       "  array(5.925471, dtype=float32),\n",
       "  array(7.2543826, dtype=float32),\n",
       "  array(6.6150303, dtype=float32),\n",
       "  array(5.3538632, dtype=float32),\n",
       "  array(7.8501463, dtype=float32),\n",
       "  array(6.1681023, dtype=float32),\n",
       "  array(5.999814, dtype=float32),\n",
       "  array(5.8146586, dtype=float32),\n",
       "  array(7.5268273, dtype=float32),\n",
       "  array(6.5300856, dtype=float32),\n",
       "  array(5.4118943, dtype=float32),\n",
       "  array(7.8613915, dtype=float32),\n",
       "  array(6.8065734, dtype=float32),\n",
       "  array(6.919258, dtype=float32),\n",
       "  array(6.2770457, dtype=float32),\n",
       "  array(6.4420443, dtype=float32),\n",
       "  array(7.208762, dtype=float32),\n",
       "  array(6.6041107, dtype=float32),\n",
       "  array(7.575796, dtype=float32),\n",
       "  array(5.769201, dtype=float32),\n",
       "  array(7.659681, dtype=float32),\n",
       "  array(6.986711, dtype=float32),\n",
       "  array(6.5490265, dtype=float32),\n",
       "  array(7.2186213, dtype=float32),\n",
       "  array(8.113386, dtype=float32),\n",
       "  array(6.852587, dtype=float32),\n",
       "  array(6.1701875, dtype=float32),\n",
       "  array(6.306319, dtype=float32),\n",
       "  array(7.00372, dtype=float32),\n",
       "  array(6.8989897, dtype=float32),\n",
       "  array(7.3526287, dtype=float32),\n",
       "  array(6.8399544, dtype=float32),\n",
       "  array(7.3346148, dtype=float32),\n",
       "  array(9.462653, dtype=float32),\n",
       "  array(6.9749265, dtype=float32),\n",
       "  array(7.182026, dtype=float32),\n",
       "  array(6.0577664, dtype=float32),\n",
       "  array(7.7704, dtype=float32),\n",
       "  array(6.3684263, dtype=float32),\n",
       "  array(7.291076, dtype=float32),\n",
       "  array(6.332697, dtype=float32),\n",
       "  array(8.930985, dtype=float32),\n",
       "  array(6.7580366, dtype=float32),\n",
       "  array(7.753331, dtype=float32),\n",
       "  array(6.052797, dtype=float32),\n",
       "  array(7.921232, dtype=float32),\n",
       "  array(7.256372, dtype=float32),\n",
       "  array(7.234798, dtype=float32),\n",
       "  array(9.317127, dtype=float32),\n",
       "  array(6.624877, dtype=float32),\n",
       "  array(5.5154943, dtype=float32),\n",
       "  array(6.666352, dtype=float32),\n",
       "  array(7.109341, dtype=float32),\n",
       "  array(8.269861, dtype=float32),\n",
       "  array(5.3578534, dtype=float32),\n",
       "  array(8.178148, dtype=float32),\n",
       "  array(6.4589453, dtype=float32),\n",
       "  array(6.68044, dtype=float32),\n",
       "  array(6.138837, dtype=float32),\n",
       "  array(7.0078564, dtype=float32),\n",
       "  array(7.2320695, dtype=float32),\n",
       "  array(8.345342, dtype=float32),\n",
       "  array(7.0202246, dtype=float32),\n",
       "  array(8.343303, dtype=float32),\n",
       "  array(7.111789, dtype=float32),\n",
       "  array(8.352723, dtype=float32),\n",
       "  array(7.2790184, dtype=float32),\n",
       "  array(7.0776005, dtype=float32),\n",
       "  array(6.180545, dtype=float32),\n",
       "  array(7.141199, dtype=float32),\n",
       "  array(6.9591203, dtype=float32),\n",
       "  array(5.1738505, dtype=float32),\n",
       "  array(6.301614, dtype=float32),\n",
       "  array(6.8643255, dtype=float32),\n",
       "  array(5.971078, dtype=float32),\n",
       "  array(6.6851106, dtype=float32),\n",
       "  array(6.9253025, dtype=float32),\n",
       "  array(7.5044217, dtype=float32),\n",
       "  array(6.2778563, dtype=float32),\n",
       "  array(7.0163856, dtype=float32),\n",
       "  array(6.7962227, dtype=float32),\n",
       "  array(5.7766137, dtype=float32),\n",
       "  array(7.068201, dtype=float32),\n",
       "  array(7.065934, dtype=float32),\n",
       "  array(7.400505, dtype=float32),\n",
       "  array(8.124908, dtype=float32),\n",
       "  array(8.897418, dtype=float32),\n",
       "  array(6.961408, dtype=float32),\n",
       "  array(7.491146, dtype=float32),\n",
       "  array(6.7905283, dtype=float32),\n",
       "  array(8.150234, dtype=float32),\n",
       "  array(7.8819075, dtype=float32),\n",
       "  array(7.7058873, dtype=float32),\n",
       "  array(7.9684424, dtype=float32),\n",
       "  array(7.026542, dtype=float32),\n",
       "  array(7.03156, dtype=float32),\n",
       "  array(7.390721, dtype=float32),\n",
       "  array(7.1660466, dtype=float32),\n",
       "  array(7.574244, dtype=float32),\n",
       "  array(7.6234365, dtype=float32),\n",
       "  array(9.836409, dtype=float32),\n",
       "  array(7.2159452, dtype=float32),\n",
       "  array(6.652216, dtype=float32),\n",
       "  array(8.095136, dtype=float32),\n",
       "  array(7.3776193, dtype=float32),\n",
       "  array(6.722825, dtype=float32),\n",
       "  array(9.086422, dtype=float32),\n",
       "  array(7.502506, dtype=float32),\n",
       "  array(9.128791, dtype=float32),\n",
       "  array(6.8301845, dtype=float32),\n",
       "  array(6.877135, dtype=float32),\n",
       "  array(7.014467, dtype=float32),\n",
       "  array(7.2685595, dtype=float32),\n",
       "  array(7.120498, dtype=float32),\n",
       "  array(7.450802, dtype=float32),\n",
       "  array(6.832544, dtype=float32),\n",
       "  array(8.104681, dtype=float32),\n",
       "  array(6.7830877, dtype=float32),\n",
       "  array(7.078164, dtype=float32),\n",
       "  array(8.137202, dtype=float32),\n",
       "  array(6.845503, dtype=float32),\n",
       "  array(6.298464, dtype=float32),\n",
       "  array(7.469415, dtype=float32),\n",
       "  array(7.0331974, dtype=float32),\n",
       "  array(7.7102427, dtype=float32),\n",
       "  array(9.220516, dtype=float32),\n",
       "  array(7.1712284, dtype=float32),\n",
       "  array(7.208781, dtype=float32),\n",
       "  array(7.263051, dtype=float32),\n",
       "  array(5.5915303, dtype=float32),\n",
       "  array(8.574799, dtype=float32),\n",
       "  array(6.3855214, dtype=float32),\n",
       "  array(7.8618126, dtype=float32),\n",
       "  array(7.9529347, dtype=float32),\n",
       "  array(7.0998526, dtype=float32),\n",
       "  array(6.8006973, dtype=float32),\n",
       "  array(8.032468, dtype=float32),\n",
       "  array(8.0043335, dtype=float32),\n",
       "  array(7.453384, dtype=float32),\n",
       "  array(8.030354, dtype=float32),\n",
       "  array(6.6694546, dtype=float32),\n",
       "  array(7.706446, dtype=float32),\n",
       "  array(6.2676826, dtype=float32),\n",
       "  array(7.085862, dtype=float32),\n",
       "  array(7.065719, dtype=float32),\n",
       "  array(7.9989734, dtype=float32),\n",
       "  array(7.5699706, dtype=float32),\n",
       "  array(7.127736, dtype=float32),\n",
       "  array(6.759207, dtype=float32),\n",
       "  array(6.106082, dtype=float32),\n",
       "  array(7.520642, dtype=float32),\n",
       "  array(8.036333, dtype=float32),\n",
       "  array(9.083107, dtype=float32),\n",
       "  array(6.5881367, dtype=float32),\n",
       "  array(7.977955, dtype=float32),\n",
       "  array(7.7748284, dtype=float32),\n",
       "  array(7.705992, dtype=float32),\n",
       "  array(9.5059395, dtype=float32),\n",
       "  array(7.2900786, dtype=float32),\n",
       "  array(7.3050165, dtype=float32),\n",
       "  array(9.155029, dtype=float32),\n",
       "  array(8.835761, dtype=float32),\n",
       "  array(9.172257, dtype=float32),\n",
       "  array(9.903617, dtype=float32),\n",
       "  array(8.126141, dtype=float32),\n",
       "  array(8.277916, dtype=float32),\n",
       "  array(7.612353, dtype=float32),\n",
       "  array(7.0929375, dtype=float32),\n",
       "  array(7.038781, dtype=float32),\n",
       "  array(7.4444494, dtype=float32),\n",
       "  array(9.0479145, dtype=float32),\n",
       "  array(7.431265, dtype=float32),\n",
       "  array(8.17844, dtype=float32),\n",
       "  array(8.905776, dtype=float32),\n",
       "  array(5.812213, dtype=float32),\n",
       "  array(7.4540653, dtype=float32),\n",
       "  array(8.075509, dtype=float32),\n",
       "  array(7.2250566, dtype=float32),\n",
       "  array(9.056469, dtype=float32),\n",
       "  array(6.6022763, dtype=float32),\n",
       "  array(6.6315284, dtype=float32),\n",
       "  array(7.7460113, dtype=float32),\n",
       "  array(6.5366144, dtype=float32),\n",
       "  array(6.3500137, dtype=float32),\n",
       "  array(7.4653234, dtype=float32),\n",
       "  array(7.0189457, dtype=float32),\n",
       "  array(9.53687, dtype=float32),\n",
       "  array(8.041086, dtype=float32),\n",
       "  array(8.126538, dtype=float32),\n",
       "  array(6.9126883, dtype=float32),\n",
       "  array(6.6462746, dtype=float32),\n",
       "  array(7.4472694, dtype=float32),\n",
       "  array(6.814092, dtype=float32),\n",
       "  array(7.7534127, dtype=float32),\n",
       "  array(7.917623, dtype=float32),\n",
       "  array(8.192478, dtype=float32),\n",
       "  array(9.490717, dtype=float32),\n",
       "  array(7.8000555, dtype=float32),\n",
       "  array(8.8866625, dtype=float32),\n",
       "  array(7.851291, dtype=float32),\n",
       "  array(8.46692, dtype=float32),\n",
       "  array(7.8011665, dtype=float32),\n",
       "  array(8.72133, dtype=float32),\n",
       "  array(7.2780933, dtype=float32),\n",
       "  array(7.814337, dtype=float32),\n",
       "  array(8.532968, dtype=float32),\n",
       "  array(7.952024, dtype=float32),\n",
       "  array(9.002746, dtype=float32),\n",
       "  array(8.265585, dtype=float32),\n",
       "  array(7.7197556, dtype=float32),\n",
       "  array(8.443336, dtype=float32),\n",
       "  array(9.801701, dtype=float32),\n",
       "  array(6.651739, dtype=float32),\n",
       "  array(7.148577, dtype=float32),\n",
       "  array(8.179709, dtype=float32),\n",
       "  array(9.330074, dtype=float32),\n",
       "  array(9.744348, dtype=float32),\n",
       "  array(8.779357, dtype=float32),\n",
       "  array(7.4250073, dtype=float32),\n",
       "  array(7.5842557, dtype=float32),\n",
       "  array(8.581951, dtype=float32),\n",
       "  array(9.23951, dtype=float32),\n",
       "  array(7.8870153, dtype=float32),\n",
       "  array(7.422641, dtype=float32),\n",
       "  array(7.475436, dtype=float32),\n",
       "  array(8.495918, dtype=float32),\n",
       "  array(9.990104, dtype=float32),\n",
       "  array(6.7663097, dtype=float32),\n",
       "  array(8.3785, dtype=float32),\n",
       "  array(9.299452, dtype=float32),\n",
       "  array(9.134307, dtype=float32),\n",
       "  array(7.178063, dtype=float32),\n",
       "  array(9.696802, dtype=float32),\n",
       "  array(9.249541, dtype=float32),\n",
       "  array(7.75998, dtype=float32),\n",
       "  array(8.665875, dtype=float32),\n",
       "  array(6.095452, dtype=float32),\n",
       "  array(8.794637, dtype=float32),\n",
       "  array(6.809418, dtype=float32),\n",
       "  array(9.525933, dtype=float32),\n",
       "  array(7.7255716, dtype=float32),\n",
       "  array(7.489865, dtype=float32),\n",
       "  array(8.429841, dtype=float32),\n",
       "  array(9.430864, dtype=float32),\n",
       "  array(6.9786286, dtype=float32),\n",
       "  array(5.9034104, dtype=float32),\n",
       "  array(10.090929, dtype=float32),\n",
       "  array(7.7938952, dtype=float32),\n",
       "  array(9.560843, dtype=float32),\n",
       "  array(9.316346, dtype=float32),\n",
       "  array(8.963377, dtype=float32),\n",
       "  array(9.016125, dtype=float32),\n",
       "  array(7.4195247, dtype=float32),\n",
       "  array(7.9590445, dtype=float32),\n",
       "  array(7.29319, dtype=float32),\n",
       "  array(8.768692, dtype=float32),\n",
       "  array(7.858839, dtype=float32),\n",
       "  array(9.493994, dtype=float32),\n",
       "  array(9.952677, dtype=float32),\n",
       "  array(9.680899, dtype=float32),\n",
       "  array(6.592593, dtype=float32),\n",
       "  array(9.448979, dtype=float32),\n",
       "  array(10.133996, dtype=float32),\n",
       "  array(8.311864, dtype=float32),\n",
       "  array(11.745404, dtype=float32),\n",
       "  array(9.308321, dtype=float32),\n",
       "  array(8.341458, dtype=float32),\n",
       "  array(9.608872, dtype=float32),\n",
       "  array(8.4090395, dtype=float32),\n",
       "  array(9.039004, dtype=float32),\n",
       "  array(8.990931, dtype=float32),\n",
       "  array(8.434843, dtype=float32),\n",
       "  array(9.823877, dtype=float32),\n",
       "  array(8.514258, dtype=float32),\n",
       "  array(9.705085, dtype=float32),\n",
       "  array(10.95734, dtype=float32),\n",
       "  array(7.5163417, dtype=float32),\n",
       "  array(8.315553, dtype=float32),\n",
       "  array(7.848273, dtype=float32),\n",
       "  array(10.260522, dtype=float32),\n",
       "  array(7.9458447, dtype=float32),\n",
       "  array(8.623998, dtype=float32),\n",
       "  array(8.874177, dtype=float32),\n",
       "  array(9.877287, dtype=float32),\n",
       "  array(9.396085, dtype=float32),\n",
       "  array(8.793123, dtype=float32),\n",
       "  array(9.861873, dtype=float32),\n",
       "  array(9.834271, dtype=float32),\n",
       "  array(11.590424, dtype=float32),\n",
       "  array(6.9121795, dtype=float32),\n",
       "  array(9.72543, dtype=float32),\n",
       "  array(9.365141, dtype=float32),\n",
       "  array(8.683455, dtype=float32),\n",
       "  array(11.263447, dtype=float32),\n",
       "  array(9.694299, dtype=float32),\n",
       "  array(7.0971866, dtype=float32),\n",
       "  array(10.271512, dtype=float32),\n",
       "  array(11.282722, dtype=float32),\n",
       "  array(10.132987, dtype=float32),\n",
       "  array(9.106739, dtype=float32),\n",
       "  array(9.701689, dtype=float32),\n",
       "  array(11.459127, dtype=float32),\n",
       "  array(8.345947, dtype=float32),\n",
       "  array(9.310978, dtype=float32),\n",
       "  array(9.262587, dtype=float32),\n",
       "  array(13.305407, dtype=float32),\n",
       "  array(6.1380167, dtype=float32),\n",
       "  array(9.4412365, dtype=float32),\n",
       "  array(9.636497, dtype=float32),\n",
       "  array(10.040654, dtype=float32),\n",
       "  array(8.511625, dtype=float32),\n",
       "  array(10.713156, dtype=float32),\n",
       "  array(8.072149, dtype=float32),\n",
       "  array(8.413536, dtype=float32),\n",
       "  array(8.237611, dtype=float32),\n",
       "  array(9.076281, dtype=float32),\n",
       "  array(7.764576, dtype=float32),\n",
       "  array(10.076089, dtype=float32),\n",
       "  array(9.038959, dtype=float32),\n",
       "  array(11.168418, dtype=float32),\n",
       "  array(9.857673, dtype=float32),\n",
       "  array(10.925979, dtype=float32),\n",
       "  array(9.841288, dtype=float32),\n",
       "  array(9.243246, dtype=float32),\n",
       "  array(9.196756, dtype=float32),\n",
       "  array(10.028124, dtype=float32),\n",
       "  array(8.547115, dtype=float32),\n",
       "  array(10.849496, dtype=float32),\n",
       "  array(10.061836, dtype=float32),\n",
       "  array(10.002814, dtype=float32),\n",
       "  array(11.539162, dtype=float32),\n",
       "  array(8.59359, dtype=float32),\n",
       "  array(11.550172, dtype=float32),\n",
       "  array(8.714489, dtype=float32),\n",
       "  array(8.658529, dtype=float32),\n",
       "  array(9.036378, dtype=float32),\n",
       "  array(8.539797, dtype=float32),\n",
       "  array(12.170733, dtype=float32),\n",
       "  array(10.043983, dtype=float32),\n",
       "  array(10.362089, dtype=float32),\n",
       "  array(10.712949, dtype=float32),\n",
       "  array(9.855534, dtype=float32),\n",
       "  array(9.51585, dtype=float32),\n",
       "  array(11.0043, dtype=float32),\n",
       "  array(10.429231, dtype=float32),\n",
       "  array(11.51613, dtype=float32),\n",
       "  array(11.963815, dtype=float32),\n",
       "  array(9.121792, dtype=float32),\n",
       "  array(13.899215, dtype=float32),\n",
       "  array(9.344667, dtype=float32),\n",
       "  array(9.388119, dtype=float32),\n",
       "  array(8.23044, dtype=float32),\n",
       "  array(11.702668, dtype=float32),\n",
       "  array(9.09219, dtype=float32),\n",
       "  array(9.725459, dtype=float32),\n",
       "  array(10.602994, dtype=float32),\n",
       "  array(11.772246, dtype=float32),\n",
       "  array(10.44131, dtype=float32),\n",
       "  array(9.862541, dtype=float32),\n",
       "  array(13.521267, dtype=float32),\n",
       "  array(10.185733, dtype=float32),\n",
       "  array(11.343135, dtype=float32),\n",
       "  array(11.436339, dtype=float32),\n",
       "  array(10.3712015, dtype=float32),\n",
       "  array(10.767947, dtype=float32),\n",
       "  array(9.042661, dtype=float32),\n",
       "  array(7.839087, dtype=float32),\n",
       "  array(8.99128, dtype=float32),\n",
       "  array(12.873168, dtype=float32),\n",
       "  array(14.830836, dtype=float32),\n",
       "  array(11.178264, dtype=float32),\n",
       "  array(12.5908985, dtype=float32),\n",
       "  array(11.078297, dtype=float32),\n",
       "  array(11.128754, dtype=float32),\n",
       "  array(10.668614, dtype=float32),\n",
       "  array(9.744959, dtype=float32),\n",
       "  array(10.528398, dtype=float32),\n",
       "  array(9.54645, dtype=float32),\n",
       "  array(9.671074, dtype=float32),\n",
       "  array(9.602103, dtype=float32),\n",
       "  array(10.932723, dtype=float32),\n",
       "  array(10.913155, dtype=float32),\n",
       "  array(11.438326, dtype=float32),\n",
       "  array(14.088532, dtype=float32),\n",
       "  array(13.287795, dtype=float32),\n",
       "  array(12.512604, dtype=float32),\n",
       "  array(11.316035, dtype=float32),\n",
       "  array(10.736258, dtype=float32),\n",
       "  array(10.356643, dtype=float32),\n",
       "  array(9.728119, dtype=float32),\n",
       "  array(8.9214525, dtype=float32),\n",
       "  ...],\n",
       " [array(0.63431382)])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "\n",
    "\n",
    "data = TensorDataset(t.LongTensor(eng_seq_collection[:-1]).to(device),\n",
    "                               t.LongTensor(guj_seq_collection[:-1]).to(device))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = random_split(data, [0.7, 0.3])\n",
    "\n",
    "\n",
    "# Set up Training & Validation DataLoaders\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# Run Training Loop\n",
    "train(train_dataloader, val_dataloader, encoder, decoder, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the output. The token can generate many different tokens.\n",
    "# Generator stage: hidden tensor to vocab tensor, and then softmax to get probablitiy\n",
    "# Greedy search is saying we want the highest probability token to be the word\n",
    "\n",
    "# v.s., the greedy search at that time might not be the best at predicting the right word\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/cqrdw531507g4cfv8ncd8fz80000gp/T/ipykernel_1216/2293439555.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = t.tensor(t.ones(batch_size)*decoder_input_int).long()\n"
     ]
    }
   ],
   "source": [
    "# # Capture the output in 2 different objects\n",
    "# # note: encoder_output_hidden[1] is context vector\n",
    "# # output, hidden = decoder(encoder_output_hidden[1][-1].unsqueeze(0)) # the -1 is because we need the output from the 2nd LSTM layer\n",
    "# # the unsqueeze adds a dummy index (adds one more dimension)\n",
    "# # unsqueeze --> [seq_length, batch_size]\n",
    "\n",
    "# # need to transpose bc batch size needs to be first\n",
    "# # transpose --> [batch_size, seq_len]\n",
    "# # encoder_out shape: [batch_size, num_layers, hidden_size]\n",
    "# encoder_out = t.transpose(encoder_output_hidden[1], 0, 1)\n",
    "# output, hidden = decoder(encoder_out)\n",
    "\n",
    "\n",
    "# # hidden output (short term memory). tensor[#_layers, batch_size, hidden_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 55])\n",
      "6500\n"
     ]
    }
   ],
   "source": [
    "# # Prep the real data\n",
    "# encoder_input = t.tensor(eng_seq_collection)\n",
    "# len(encoder_input)\n",
    "\n",
    "# # split the encoder input into batches\n",
    "# i = 0\n",
    "# j = 10\n",
    "# encoder_input_list = []\n",
    "# while j <= 65002:\n",
    "#     encoder_input_list.append(encoder_input[i:j])\n",
    "#     i = j\n",
    "#     j += 10\n",
    "\n",
    "\n",
    "# # Now we have 6500 tensors with dimensions batch_size, seq_len\n",
    "# print(encoder_input_list[0].shape)\n",
    "# print(len(encoder_input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'A', 'bicycle', 'replica', 'with', 'a', 'clock', 'as', 'the', 'front', 'wheel.', '</s>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>']\n",
      "['બિંબ', 'બ્લેડ', 'દ્રશ્યની', 'વસ્તુની', 'મિશ્રણના', 'ગયા', 'પ્લેટફોર્મ્સની', 'બદલ્યો.', 'મિશ્રણના', 'ગયા', 'પીળાં', 'sleek', 'ગ્રેશ', 'હેન્ડલને', 'રહેણાંક', 'સ્કેટબોર્ડર્સ', 'બિલ્ટ-ઇન', 'ગયા', 'પંક્તિમાં', 'ગયા', 'એકલા', 'દીવાદાંડીના', 'આંતરડા', 'થયેલ.', 'રસોઇ,', 'બેસપૅક', 'ટોનથી', 'countertop.', 'ગિરરફેસનું', 'બૂટ્સમાં', 'એવન્યુ\"', 'દર્શાવવામાં', 'વાડોમાં', 'સ્ટેજીંગ', 'પેસેન્જર્સને', 'તિરાડ', 'ગ્રેફિટીમાં', 'મિશ્રણના', 'ગયા', 'વાદળાંઓના', 'ઢાંકતી', 'ઝાડવુંની', 'છોડના', 'મિશ્રણના', 'જીરાફના', 'આંતરડા', 'રંગની', 'બાંદનામાં', 'ગયા', 'કંટ્રી']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/cqrdw531507g4cfv8ncd8fz80000gp/T/ipykernel_1216/2293439555.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = t.tensor(t.ones(batch_size)*decoder_input_int).long()\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "# Capture the output in 2 different objects\n",
    "\n",
    "# encoder_output_total = t.empty(10, 56, 128)\n",
    "# encoder_output_hidden_total = t.cat((t.empty(2, 10, 128), t.empty(2, 10, 128)))\n",
    "for encoder_input_batch in encoder_input_list[:1]:\n",
    "    encoder_output, encoder_output_hidden = encoder(encoder_input_batch)\n",
    "    encoder_out = t.transpose(encoder_output_hidden[1], 0, 1)\n",
    "    output, hidden = decoder(encoder_out)\n",
    "    predictions = output.argmax(dim=-1)\n",
    "    x = map(lambda x: guj_index2word[x], predictions[0].tolist())\n",
    "    y = map(lambda x: eng_index2word[x], encoder_input_batch[0].detach().tolist())\n",
    "    print(list(y))\n",
    "    print(list(x))\n",
    "    break\n",
    "    # encoder_output_total = t.stack((encoder_output_total, encoder_output))\n",
    "    # encoder_output_hidden_total = t.stack((encoder_output_hidden_total, encoder_output_hidden))\n",
    "    # encoder_output_list.append(encoder_output)\n",
    "    # encoder_output_hidden_list.append(encoder_output_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'A', 'bicycle', 'replica', 'with', 'a', 'clock', 'as', 'the', 'front', 'wheel.', '</s>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>']\n",
      "['બિંબ', 'બ્લેડ', 'બલૂન.', 'ડિસ્ટ્રિક્ટ', 'હાથીઓને', 'ઝાપટિયું', '\"ફેક્ટરી', 'ગયા', 'બદલ્યો.', 'ટ્રોલી', 'મશરૂમથી', 'ગયા', 'થયેલ.', 'બલૂન.', '\"ટિપ્સ', '\"ફેક્ટરી', 'થયેલ.', 'મેટલની', 'મિશ્રણના', 'પવનમ', 'ડ્રોપ્સ', 'તિરાડ', 'બદલ્યો.', 'મિશ્રણના', 'ગયા', 'તિરાડ', 'બદલ્યો.', 'મિશ્રણના', 'પહોળાઇને', 'તિરાડ', 'ગયા', 'બેર', 'વાડોમાં', 'બફેટ', 'એન્જિનના', 'મોટા', 'વાડોમાં', 'ક્ષેત્રને', 'કૂદકા.', 'એડ્સ', 'વાડોમાં', 'કૂતરાના', 'બસ્સી', 'પહોળાઇને', 'સ્ટે', 'સપાટી,', 'સ્પાઘેટ્ટી', 'ગયા', 'બલૂન.', 'વાડોમાં']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/cqrdw531507g4cfv8ncd8fz80000gp/T/ipykernel_1216/2293439555.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = t.tensor(t.ones(batch_size)*decoder_input_int).long()\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "# Capture the output in 2 different objects\n",
    "\n",
    "# encoder_output_total = t.empty(10, 56, 128)\n",
    "# encoder_output_hidden_total = t.cat((t.empty(2, 10, 128), t.empty(2, 10, 128)))\n",
    "for encoder_input_batch in encoder_input_list[:1]:\n",
    "    encoder_output, encoder_output_hidden = encoder(encoder_input_batch)\n",
    "    encoder_out = t.transpose(encoder_output_hidden[1], 0, 1)\n",
    "    output, hidden = decoder(encoder_out)\n",
    "    predictions = output.argmax(dim=-1)\n",
    "    x = map(lambda x: guj_index2word[x], predictions[0].tolist())\n",
    "    y = map(lambda x: eng_index2word[x], encoder_input_batch[0].detach().tolist())\n",
    "    print(list(y))\n",
    "    print(list(x))\n",
    "    break\n",
    "    # encoder_output_total = t.stack((encoder_output_total, encoder_output))\n",
    "    # encoder_output_hidden_total = t.stack((encoder_output_hidden_total, encoder_output_hidden))\n",
    "    # encoder_output_list.append(encoder_output)\n",
    "    # encoder_output_hidden_list.append(encoder_output_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'A', 'bicycle', 'replica', 'with', 'a', 'clock', 'as', 'the', 'front', 'wheel.', '</s>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>']\n",
      "['બિંબ', 'બ્લેડ', 'બ્રોકોલીનો', '\"ફેક્ટરી', 'અકસ્માતમાં', 'લલામાસથી', 'ઈંટ', 'પ્રકારનો', 'પોચી', 'તિરાડ', 'બદલ્યો.', 'ક્લેર', 'સ્ટેટિક', 'ઉભરાઇ', 'ડાર્કમેકની', 'ગયા', 'પાકકળા', 'બારીઓ,', 'પ્રપોઝ', 'થયેલ.', 'ઢાંકેલો', 'ખબર', 'બદલ્યો.', 'મિશ્રણના', 'બેરલ', 'પ્લેટફોર્મ્સની', '\"ખાવું', 'ઓક્ટોપસની', 'ટ્રેક્સનું', 'લાઇન', 'નિશાનવાળી', 'સ્ટ્રક્ચર', 'સિનના', 'ક્રેશ', 'પંક્તિમાં', 'શાળાના', 'કુડુસ', 'વાડોમાં', 'ગયા', 'પીળાં', 'રંગભેદ', 'મિશ્રણના', '\"સ્ટોપ\"', 'યાદોને', 'નળીમાંથી', 'લોન', 'કેક્ટસથી', 'દંપતિનો', 'ઉપયોગ\"', 'વન્યજીવન']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/cqrdw531507g4cfv8ncd8fz80000gp/T/ipykernel_1216/2293439555.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = t.tensor(t.ones(batch_size)*decoder_input_int).long()\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "# Capture the output in 2 different objects\n",
    "\n",
    "# encoder_output_total = t.empty(10, 56, 128)\n",
    "# encoder_output_hidden_total = t.cat((t.empty(2, 10, 128), t.empty(2, 10, 128)))\n",
    "for encoder_input_batch in encoder_input_list[:1]:\n",
    "    encoder_output, encoder_output_hidden = encoder(encoder_input_batch)\n",
    "    encoder_out = t.transpose(encoder_output_hidden[1], 0, 1)\n",
    "    output, hidden = decoder(encoder_out)\n",
    "    predictions = output.argmax(dim=-1)\n",
    "    x = map(lambda x: guj_index2word[x], predictions[0].tolist())\n",
    "    y = map(lambda x: eng_index2word[x], encoder_input_batch[0].detach().tolist())\n",
    "    print(list(y))\n",
    "    print(list(x))\n",
    "    break\n",
    "    # encoder_output_total = t.stack((encoder_output_total, encoder_output))\n",
    "    # encoder_output_hidden_total = t.stack((encoder_output_hidden_total, encoder_output_hidden))\n",
    "    # encoder_output_list.append(encoder_output)\n",
    "    # encoder_output_hidden_list.append(encoder_output_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[eng_tensor, gujtensor], [eng_tensor, gujtensor]]\n",
    "# \"dataloader\" = list of tuples (eng, guj) as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at dataloader from transformers/GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples with [(english, gujarati)]\n",
    "sentence_pairs = []\n",
    "for i in range(len(eng_seq_collection)):\n",
    "    sentence_tuple = (eng_seq_collection[i], guj_seq_collection[i])\n",
    "    sentence_pairs.append(sentence_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 25460,\n",
       " 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</પેડ>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guj_index2word[25460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# use randomsampler to split data into train and test (once it's in the paired tuples)\n",
    "# establish dataloader from training and test data\n",
    "# use code in the trasorer notebook (and solutions) to write train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'A', 'bicycle', 'replica', 'with', 'a', 'clock', 'as', 'the', 'front', 'wheel.', '</s>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>']\n",
      "['<s>', 'ફ્રન્ટ', 'વ્હીલ', 'તરીકે', 'ઘડિયાળ', 'સાથે', 'સાયકલ', 'પ્રતિકૃતિ.', '</s>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>', '</પેડ>']\n"
     ]
    }
   ],
   "source": [
    "x = map(lambda x: eng_index2word[x], sentence_pairs[0][0])\n",
    "y = map(lambda x: guj_index2word[x], sentence_pairs[0][1])\n",
    "\n",
    "\n",
    "print(list(x))\n",
    "print(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</pad>'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_index2word[17472]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_seq_collection[6500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in eng_seq_collection[6500]:\n",
    "    eng_index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, random_split\n",
    "data = TensorDataset(t.LongTensor(eng_seq_collection[:-1]).to(device),\n",
    "                               t.LongTensor(guj_seq_collection[:-1]).to(device))\n",
    "\n",
    "train_dataset, val_dataset = random_split(data, [0.7, 0.3])\n",
    "\n",
    "\n",
    "# Set up Training & Validation DataLoaders\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(len(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m random_element \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(data)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m(random_element)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_element = random.choice(data)\n",
    "data.drop(random_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_seq_collection[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,     3,     4,  ..., 17472, 17472, 17472],\n",
      "        [    1,     3,    13,  ..., 17472, 17472, 17472],\n",
      "        [    1,     3,    20,  ..., 17472, 17472, 17472],\n",
      "        ...,\n",
      "        [    1,     3,   461,  ..., 17472, 17472, 17472],\n",
      "        [    1,     3,   731,  ..., 17472, 17472, 17472],\n",
      "        [    1,     3,   157,  ..., 17472, 17472, 17472]])\n"
     ]
    }
   ],
   "source": [
    "#len(data[:-1][0])\n",
    "\n",
    "print(data[:-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x17fafbfe0>\n"
     ]
    }
   ],
   "source": [
    "#len(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65001"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dataloader(batch_size):\n",
    "#     input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "#     n = len(pairs)\n",
    "#     input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "#     target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "#     for idx, (inp, tgt) in enumerate(pairs):\n",
    "#         inp_ids = indexesFromSentence(input_lang, inp)\n",
    "#         tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "#         inp_ids.append(EOS_token)\n",
    "#         tgt_ids.append(EOS_token)\n",
    "#         input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "#         target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "#     train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "#                                torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "#     train_sampler = RandomSampler(train_data)\n",
    "#     train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "#     return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/cqrdw531507g4cfv8ncd8fz80000gp/T/ipykernel_98869/2013414216.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = t.tensor(t.ones(batch_size)*decoder_input_int).long()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'A', 'bicycle', 'replica', 'with', 'a', 'clock', 'as', 'the', 'front', 'wheel.', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</pad>', '</s>']\n",
      "['ફળનો', 'આડ', 'કટિંગ', 'દંપતિ.', 'પ્રાઇઝ', 'દૂધનો', 'વાહનને', 'flying', 'પોટમાં', 'ખાય', 'દૂધનો', 'ટ્રે', 'ઝાકળવાળો', 'પેચની', 'બેરિન.', 'લાકડા-પેપર.', 'ગાજરની', '200', 'ટ્રેડેડ', 'આકાશગંગા', 'ઘરઆંગણે', 'juts.', 'જીરાફ.', 'શૉલ્ફ', 'અઢારમી', 'બ્લોઇંગ', 'ફલોર,', 'ગાજરની', 'સાધનસામગ્રી', 'દૂધનો', 'પ્રતિબિંબવાળી', 'સડેલા', 'દૂધનો', 'sleek', 'આયરન.', 'flying', 'સ્વાતંત્ર્યની', 'કાર્યકર્તા.', 'ભૂખરું', 'વધારી', 'સુયોજન', 'આયરન.', 'ડ્રેપેપાઇપ', 'સ્વાતંત્ર્યની', 'સેન્ટર', 'બિયેત', '-40', 'હાઇડ્રન્ટને', 'વધઘટ', 'હેડર્રોવ']\n"
     ]
    }
   ],
   "source": [
    "for encoder_input_batch in encoder_input_list[:1]:\n",
    "    encoder_output, encoder_output_hidden = encoder(encoder_input_batch)\n",
    "    encoder_out = t.transpose(encoder_output_hidden[1], 0, 1)\n",
    "    output, hidden = decoder(encoder_out)\n",
    "    predictions = output.argmax(dim=-1)\n",
    "    x = map(lambda x: guj_index2word[x], predictions[0].tolist())\n",
    "    y = map(lambda x: eng_index2word[x], encoder_input_batch[0].detach().tolist())\n",
    "    print(list(y))\n",
    "    print(list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "def train_epoch(epoch_number, dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader, desc=\"Epoch Number {}\".format(epoch_number)):\n",
    "\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        # if input_tensor.shape != t.Size([10, 55]):\n",
    "        #     print(input_tensor.shape)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_output, encoder_output_hidden = encoder(input_tensor)\n",
    "        encoder_out = t.transpose(encoder_output_hidden[1], 0, 1)\n",
    "\n",
    "        # check size of all encoder_outputs\n",
    "        # assert encoder_out.shape == t.Size([10, 2, 128])\n",
    "\n",
    "        output, hidden = decoder(encoder_out)\n",
    "\n",
    "        loss = criterion(\n",
    "            output.view(-1, output.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        # print(loss)\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # See loss for each batch\n",
    "        \n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss() #negative log likelihood loss\n",
    "\n",
    "    #with tqdm(range(1, n_epochs + 1)) as epoch:\n",
    "    # for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "    for epoch_number in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(epoch_number, train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch_number % 100 == 0:\n",
    "            print_loss_avg = print_loss_total / 100\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch_number / n_epochs),\n",
    "                                        epoch_number, epoch_number / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch_number % 100 == 0:\n",
    "            plot_loss_avg = plot_loss_total / 100\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    #return plot_losses\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch Number 1:   0%|          | 0/20 [00:00<?, ?it/s]/var/folders/ww/cqrdw531507g4cfv8ncd8fz80000gp/T/ipykernel_98869/2293439555.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_input = t.tensor(t.ones(batch_size)*decoder_input_int).long()\n",
      "Epoch Number 1: 100%|██████████| 20/20 [00:19<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 19s (- 0m 0s) (1 100%) 1.9910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(train_dataloader, encoder, decoder, n_epochs=1, print_every=1, plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataloader:\n",
    "    print(len(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
    "    '''\n",
    "    Calculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\n",
    "\n",
    "    Remember that `batch` is a dictionary with the single key 'tokens'.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    tokens = batch[\"tokens\"].to(device)\n",
    "    output = self.model(tokens)\n",
    "    loss = -get_log_probs(output, tokens).mean()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    self.optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "def validation_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]):\n",
    "    '''\n",
    "    Calculates & returns the accuracy on the tokens in the batch (i.e. how often the model's prediction\n",
    "    is correct). Logging should happen in the `train` function (after we've computed the accuracy for\n",
    "    the whole validation set).\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    tokens = batch[\"tokens\"].to(device)\n",
    "    output = self.model(tokens)[:,:-1]\n",
    "    predictions = output.argmax(dim=-1)\n",
    "    correct = (predictions == tokens[:,1:]).flatten()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good article: https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        # d_in: embedding size\n",
    "        # d_out_qk:\n",
    "        # d_out_v: \n",
    "\n",
    "        # note: keys and values usually come from encoder and queries come from decoder\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.W_query = nn.Parameter(t.rand(d_in, d_out_kq))\n",
    "        self.W_key   = nn.Parameter(t.rand(d_in, d_out_kq))\n",
    "        self.W_value = nn.Parameter(t.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x_1, x_2):\n",
    "        # x_2 is new in cross attention\n",
    "        # note: if we set x1 = x2, this is the same as self-attention\n",
    "        # queries (decoder) are from x2 -> this is from gujarati in our case\n",
    "        # keys and values (encoder) are from x1 -> from english\n",
    "        # the attention mechanism is evaluating the interaction between two different inputs\n",
    "\n",
    "        # each context vector is a weighted sum of the values.\n",
    "        # But unlike self-attention, the values come from the 2nd input (x2) -> which comes from gujarati\n",
    "        # The weights are based on the interaction between x1 and x2\n",
    "        queries_1 = x_1 @ self.W_query\n",
    "            \n",
    "        keys_2 = x_2 @ self.W_key # new\n",
    "        values_2 = x_2 @ self.W_value# new\n",
    "            \n",
    "        attn_scores = queries_1 @ keys_2.T # new \n",
    "        attn_weights = t.softmax(\n",
    "            attn_scores / self.d_out_kq**0.5, dim=-1)\n",
    "            \n",
    "        context_vec = attn_weights @ values_2\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"test dataloader\"\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
